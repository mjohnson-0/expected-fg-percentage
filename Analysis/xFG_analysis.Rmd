---
title: "Expected Field Goal Percentage"
author: "Matt Johnson"
date: "June 2020"
output: 
  html_notebook:
    theme: paper
    toc: true
    toc_float: true
    code_folding: hide
---

The objective of this project is to develop a model for expected field goal percentage based on distance and weather conditions when available. Data comes from the `nflfastr` package between the 2010 and 2019 seasons.

```{r, message=FALSE, warning=FALSE}
## Expected Field Goal Percentage
options(dplyr.summarise.inform=F)

# Load Packages
library(nflscrapR)
library(nflfastR)
library(tidyverse)
library(mgcv)
library(lme4)
library(DT)

# Load data
seasons <- 2010:2019
pbp <- purrr::map_df(seasons, function(x) {
  readRDS(
    url(
      glue::glue("https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds")
    )
  )
})

# Clean data
kick <- pbp %>% 
  filter((extra_point_attempt == 1 | field_goal_attempt == 1)) %>% 
  mutate(kick_attempt = ifelse(extra_point_attempt == 1 | field_goal_attempt == 1, 1, 0),
         kick_result = ifelse(extra_point_attempt == 1, extra_point_result, field_goal_result),
         points_scored = score_differential_post - score_differential,
         kick_success = ifelse(points_scored > 0, 1, 0),
         roof = ifelse(roof %in% c("dome", "closed"), 1, 0),
         wind = ifelse(is.na(wind) == T, 0, wind),
         wind_rs = wind/max(wind),
         temp = ifelse(is.na(temp) == T, 75, temp),
         rain_snow = case_when(
           str_detect(weather, "^0% Chance of Rain") == T ~ 0,
           str_detect(weather, "Zero Percent Chance of Rain") == T ~ 0,
           str_detect(weather, "No chance of rain") == T ~ 0,
           str_detect(weather, "Rain") == T ~ 1,
           str_detect(weather, "rain") == T ~ 1,
           str_detect(weather, "Snow") == T ~ 1,
           str_detect(weather, "snow") == T ~ 1,
           T ~ 0,
         ),
         dist_rs = (kick_distance - min(kick_distance))/(max(kick_distance) - min(kick_distance)),
         season = as.factor(season)) %>% 
  select(kicker_player_name, kicker_player_id, kick_attempt, kick_success, kick_distance, roof, surface, temp, wind, wind_rs, dist_rs, rain_snow, weather, season)

set.seed(999)
kick <- kick[sample(nrow(kick)),]

train <- kick %>% head(dim(kick)[1]*.9)
test <- kick %>% tail(dim(kick)[1]*.1)
```

## Model Design & Validation {.tabset .tabset-pills}

In order to model the likelihood of a successful kick, I will use a generalized additive model. This model is similar in nature to a linear regression model with the primary difference being the use of a binomial family rather than a Gaussian family. This requires a link function to transform the model's results into a probability between 0 and 1. In this case, a "logit" function is appropriate.  

In order to test various variables and build the model, the data set is split into training and testing data. There are three different formulas that will be tested.  
  
  * Distance
  * Smoothed distance
  * Distance + weather conditions
  
Variable Definition:  
  
  * `kick_distance`: distance, in yards, of the kick attempt
  * `wind`: speed, in mph, of the wind; domed stadiums have wind set equal to 0  
  * `rain_snow`: indicator variable for whether there was precipitation (rain or snow) in forecast; incomplete data for some games  

For model selection, we are focusing on two things, the accuracy of the model, as measured by a Brier Score, and the consistency of the model. The consistency is tested by looking at whether the model is calibrated for all probabilities and distances. The model is not ideal if it is accurate for a portion of the probabilities, but not all of it.  
  
The differences in out-of-sample accuracy are fairly small, ranging from .1406 to .1426. The most accurate model was the full model with distance and weather.  
  
For calibration, we can look at a couple different things. First, is calibration by distance. Does the expected success probability match the actual success rate observed for a given distance? For the case of the full model where we are including more than just distance, we have to include constants for the other variables. I set `wind` = 0 and `rain_snow` = 0. Since we have no weather, we would expect the expected values to be a bit higher than the observed values where weather was a factor. The overall shape should still be similar.  
  
Next, we can check calibration by probability. Asking the question, does X% expected success actually translate to an X% success rate in observation? There is a table displaying grouped averages by 5% increments. We would expect the `predicted` column to be equal to the `actual` column if the model is well calibrated. The graph also represents this data. The red line represents a perfect match. The blue line is a modeled relationship between predicted and actual with the grouped values plotted.
  
### Model 1  

#### Model Summary
  
```{r}
g1 <- gam(data = train, 
            kick_success ~ kick_distance,
            family = binomial("logit"))

summary(g1)
```

As expected, `kick_distance` is a significant variable and decreases the probability of success with each added yard.

#### Brier Score

```{r}
test <- test %>% 
  mutate(prob = predict(g1, ., type = "response"))
#Brier Score
paste("The Brier score for Model 1 is", 
      round(Metrics::mae(test$kick_success, test$prob), 4))
```

#### Calibration

```{r}
#Validation
kick_group <- test %>% 
  group_by(kick_distance) %>% 
  summarise(success = mean(kick_success), n = n()) %>% 
  ungroup() %>% 
  mutate(prob = predict(g1, ., type = "response"))

ggplot(data = kick_group, aes(x = kick_distance)) +
  geom_line(aes(y = success, color = "Actual")) +
  geom_line(aes(y = prob, color = "Model")) +
  labs(color = "Probability",
       title = "Success by Kick Distance")

validation <- test %>% 
  mutate(group = case_when(
    between(prob, .95, 1) ~ "95-100",
    between(prob, .90,.95) ~ "90-95",
    between(prob, .85,.90) ~ "85-90",
    between(prob, .80,.85) ~ "80-85",
    between(prob, .75,.80) ~ "75-80",
    between(prob, .70,.75) ~ "70-75",
    between(prob, .65,.70) ~ "65-70",
    between(prob, .60,.65) ~ "60-65",
    between(prob, .55,.60) ~ "55-60",
    between(prob, .0,.55) ~ "0-55"
  )) %>% 
  group_by(group) %>% 
  summarise(
    kicks = n(),
    actual = mean(kick_success),
    predicted = mean(prob)
  ) %>% 
  ungroup() %>% 
  arrange(desc(predicted))

datatable(validation,
          rownames = FALSE,
          options = list(
            dom = 't',
            columnDefs = list(list(className = 'dt-center', targets = 0:3))
          )) %>% 
  formatRound(3:4, digits = 2)

ggplot() +
  geom_smooth(data = test, aes(x = prob, y = kick_success)) +
  geom_point(data = validation, aes(x = predicted, y = actual, size = kicks)) +
  geom_abline(slope = 1, intercept = 0, color = "red") + 
  coord_cartesian(xlim = c(0.3, 1), ylim = c(0.3, 1)) +
  labs(title = "xFG% vs. Observed FG%",
       x = "xFG%",
       y = "Observed FG%")
```

### Model 2  
  
#### Model Summary  
  
```{r}
g2 <- gam(data = train, 
            kick_success ~ s(kick_distance),
            family = binomial("logit"))

summary(g2)
```

#### Brier Score

```{r}
test <- test %>% 
  mutate(prob = predict(g2, ., type = "response"))
# Brier Score
paste("The Brier score for Model 2 is", 
      round(Metrics::mae(test$kick_success, test$prob), 4))
```

#### Calibration

```{r}
#Validation
kick_group <- test %>% 
  group_by(kick_distance) %>% 
  summarise(success = mean(kick_success), n = n()) %>% 
  ungroup() %>% 
  mutate(prob = predict(g2, ., type = "response"))

ggplot(data = kick_group, aes(x = kick_distance)) +
  geom_line(aes(y = success, color = "Actual")) +
  geom_line(aes(y = prob, color = "Model")) +
  labs(color = "Probability",
       title = "Success by Kick Distance")

validation <- test %>% 
  mutate(group = case_when(
    between(prob, .95, 1) ~ "95-100",
    between(prob, .90,.95) ~ "90-95",
    between(prob, .85,.90) ~ "85-90",
    between(prob, .80,.85) ~ "80-85",
    between(prob, .75,.80) ~ "75-80",
    between(prob, .70,.75) ~ "70-75",
    between(prob, .65,.70) ~ "65-70",
    between(prob, .60,.65) ~ "60-65",
    between(prob, .55,.60) ~ "55-60",
    between(prob, .0,.55) ~ "0-55"
  )) %>% 
  group_by(group) %>% 
  summarise(
    kicks = n(),
    actual = mean(kick_success),
    predicted = mean(prob)
  ) %>% 
  ungroup() %>% 
  arrange(desc(predicted))

datatable(validation,
          rownames = FALSE,
          options = list(
            dom = 't',
            columnDefs = list(list(className = 'dt-center', targets = 0:3))
          )) %>% 
  formatRound(3:4, digits = 2)

ggplot() +
  geom_smooth(data = test, aes(x = prob, y = kick_success)) +
  geom_point(data = validation, aes(x = predicted, y = actual, size = kicks)) +
  geom_abline(slope = 1, intercept = 0, color = "red") + 
  coord_cartesian(xlim = c(0.3, 1), ylim = c(0.3, 1))  +
  labs(title = "xFG% vs. Observed FG%",
       x = "xFG%",
       y = "Observed FG%")
```

### Model 3  

#### Model Summary
  
```{r}
g3 <- gam(data = train, 
            kick_success ~ kick_distance + wind + rain_snow,
            family = binomial("logit"))

summary(g3)
```

As before, kick distance moves in the expected direction where a further distance decreases the probability of success. Introducing wind and precipitation, these variables also decrease the probability of success as they increase. This also matches our prior expectation.

#### Brier Score

```{r}
test <- test %>% 
  mutate(prob = predict(g3, ., type = "response"))
# Brier Score
paste("The Brier score for Model 3 is", 
      round(Metrics::mae(test$kick_success, test$prob), 4))
```

#### Calibration

```{r}
#Validation
kick_group <- test %>% 
  group_by(kick_distance) %>% 
  summarise(success = mean(kick_success), n = n()) %>% 
  ungroup() %>% 
  mutate(wind = 0,
         rain_snow = 0) %>% 
  mutate(prob = predict(g3, ., type = "response"))

ggplot(data = kick_group, aes(x = kick_distance)) +
  geom_line(aes(y = success, color = "Actual")) +
  geom_line(aes(y = prob, color = "Model")) +
  labs(color = "Probability",
       title = "Success by Kick Distance")

validation <- test %>% 
  mutate(group = case_when(
    between(prob, .95, 1) ~ "95-100",
    between(prob, .90,.95) ~ "90-95",
    between(prob, .85,.90) ~ "85-90",
    between(prob, .80,.85) ~ "80-85",
    between(prob, .75,.80) ~ "75-80",
    between(prob, .70,.75) ~ "70-75",
    between(prob, .65,.70) ~ "65-70",
    between(prob, .60,.65) ~ "60-65",
    between(prob, .55,.60) ~ "55-60",
    between(prob, .0,.55) ~ "0-55"
  )) %>% 
  group_by(group) %>% 
  summarise(
    kicks = n(),
    actual = mean(kick_success),
    predicted = mean(prob)
  ) %>% 
  ungroup() %>% 
  arrange(desc(predicted))

datatable(validation,
          rownames = FALSE,
          options = list(
            dom = 't',
            columnDefs = list(list(className = 'dt-center', targets = 0:3))
          )) %>% 
  formatRound(3:4, digits = 2)

ggplot() +
  geom_smooth(data = test, aes(x =prob, y = kick_success)) +
  geom_point(data = validation, aes(x = predicted, y = actual, size = kicks)) +
  geom_abline(slope = 1, intercept = 0, color = "red") + 
  coord_cartesian(xlim = c(0.3, 1), ylim = c(0.3, 1))  +
  labs(title = "xFG% vs. Observed FG%",
       x = "xFG%",
       y = "Observed FG%")
```

This model is fairly well calibrated with a slight over-estimation with xFG% between 0.7 and 0.85 and under-estimation when xFG% less than 0.7. Based on accuracy and calibration, this model appears to be the best option.


## Results

Since the model does not consider who is attempting the kick, we are able to evaluate individual kickers compared to their peers. The simplest way of doing this is by finding the difference between the predicted success rate for the kicker and their actual success rate. This metric will be called FOX (field goal over expectation). The better kickers will out-perform their expectations and have a positive FOX score. The table below is filtered down to players with a minimum of 50 kick attempts in the sample.

```{r, warning = FALSE}
kickers <- kick %>% 
  mutate(kick_prob = predict(g3, ., type = "response")) %>% 
  group_by(kicker_player_name) %>% 
  summarise(
    kicks = n(),
    success_rate = round(mean(kick_success),3),
    pred_rate = round(mean(kick_prob),3)
  ) %>% 
  ungroup() %>% 
  mutate(FOX = round(success_rate - pred_rate,3)) %>% 
  filter(kicks >= 50) %>% 
  rename(kicker = kicker_player_name) %>% 
  arrange(-FOX)

datatable(kickers,
          rownames = FALSE,
          filter = "top",
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:4))
          ))

```

Unsurprisingly, Justin Tucker shows up as the best kicker, making 6.0% more kicks than expected based on the distance and weather conditions. Badgley, Bryant, Lutz, and Butker round out the top 5. Badgley has much fewer attempts than the rest of the kickers at the top of the list so there is a potential his performance regresses more than the others. I will show another way to compare kickers shortly using a different method that takes this into account.    
  
We can also sort the difference from least to greatest and find out who under-performed their expectations the most. Second round draft pick Roberto Aguayo  shows up as the worst kicker by this measure, missing his expected field goal percentage by 8.4%. Sam Ficken, Matt Gay, Austin Seibert, and Eddie Pineiro are active kickers who appear in the bottom 10.  

```{r}
ggplot(data = kickers) +
  geom_histogram(aes(x = FOX), breaks = seq(-0.09, 0.06, 0.005)) + 
  scale_y_continuous(name = "Count", breaks = seq(0,10,2)) +
  scale_x_continuous(breaks = seq(-0.08, 0.06, 0.02)) +
  labs(title = "Distribution of FOX",
       x = "FOX")
```
  
The distribution shows that the average kicker underperforms his expected field goal percentage. Since our level of analysis is at the kicker level, rather than the kick level, this makes sense. The better kickers remain in the league longer, while the poor kickers get cycled through quicker. These leads to many more "below average" kickers than "above average."  

## Kicker Effect Model

The previous model was unaware of who the kicker was when predicting the success of a kick. As we just saw in the results, the kicker matters. Justin Tucker kicking a 45 yard field goal has a different expectation than Roberto Aguayo. Using a generalized linear mixed-effect model, we can include a random effect for each kicker. As I mentioned regarding Michael Badgley, this model will include regression to the mean for each player. This will have a more significant effect on players with a couple years of experience than someone like Matt Bryant who has played over the entire sample.    

```{r}
glmer1 <- glmer(data = train, 
                kick_success ~ wind_rs + rain_snow + dist_rs + (1|kicker_player_name),
                family = binomial,
                control=glmerControl(optimizer="bobyqa",optCtrl=list(maxfun=2e5)))

arm::display(glmer1)

test <- test %>% 
  mutate(kick_prob_gam = predict(g3, ., type = "response")) %>% 
  mutate(kick_prob_glmer = predict(glmer1, ., type = "response", allow.new.levels = T))


paste("The Brier score for the GAM is", 
      round(Metrics::mae(test$kick_success, test$kick_prob_gam), 4))
paste("The Brier score for the GLMER is", 
      round(Metrics::mae(test$kick_success, test$kick_prob_glmer), 4))

```

As before we see the same consistency as the GAM when it comes to coefficient direction, more wind, precipitation, and further distance all contribute to more difficult kicks. For this model, wind speed and kick distance were rescaled to a 0 to 1 scale, something the GLMER model handles better but doesn't change the actual results. For the accuracy, including the player effect improved the accuracy of the model for the out of sample test group compared to the best GAM.  
  
For this model, the kicker was included as a random effect for the intercept. The following table displays the adjustment applied to the intercept for each kicker. Positive values will have a higher success probability on a kick, all else equal. The FOX (Field goal over expectation) is included for comparison. 


```{r}
kick_ranef <- as.data.frame(ranef(glmer1)) %>% 
  select(kicker = grp, Intercept = condval) %>% 
  mutate(Intercept = round(Intercept, 3)) %>% 
  arrange(desc(Intercept)) %>% 
  inner_join(kickers, by = "kicker") %>% 
  select(kicker, kicks, Intercept, FOX)

datatable(kick_ranef,
          rownames = FALSE,
          filter = "top",
          options = list(
            columnDefs = list(list(className = 'dt-center', targets = 0:3))
          ))
```

Again, Justin Tucker is the GOAT standing well above the rest. The following graphic highlights the difference in probability for a few key kickers from a given distance.

```{r}
kick_group_glmer <- kick %>% 
  filter(kicker_player_name %in% c("J.Tucker", "N.Novak", "D.Akers", "W.Lutz")) %>% 
  group_by(kick_distance, dist_rs, kicker_player_name) %>% 
  filter(between(kick_distance, 19, 55)) %>% 
  summarise(success = mean(kick_success), n = n()) %>% 
  ungroup() %>% 
  mutate(wind_rs = 0,
         rain_snow = 0) %>% 
  mutate(prob = predict(glmer1, ., type = "response"))

ggplot(data = kick_group_glmer, aes(x = kick_distance)) +
  geom_line(aes(y = prob, color = kicker_player_name)) +
  labs(color = "Probability",
       title = "Success by Kick Distance")
```
Between the best (Tucker) and worst (Akers) kickers, there is a ~18% difference in expectation for a 50 yard field goal. The crazy thing is there is about the same difference between Tucker and the second best kicker (Lutz), as Lutz and an average kicker (Novak).  
  
This type of model is applicable in the team setting when it comes to in-game decision making. A team would be able to use this model to have an accurate expectation for their specific kicker when it comes to fourth down decision making, rather than just relying on a general model.  
  
## Expected Points Example

Let's work out an example problem in decision making using expected points and expected field goal percentage. The conversion rate and expected points for these outcomes are estimated using the dataset from `nflfastr`, however they are simplified for the sake of the example.  
  
Scenario:  
  
  * 3rd Quarter, Tie Game, 4th & 2 from the opponent's 33 yard line  
  * Decision: Go for first down or 50 yard field goal attempt 
    
Outcomes:  
  
  * Go for it - Success  
    + For simplicity, gain 2 yards and a first down at opponent's 31 yard line  
    + 50% Conversion Rate  
    + 4.0 Expected Points
  * Go for it - Fail  
    + No gain, opponent takes over at their own 33 yard line  
    + (1 - Conversion Rate)  
    + -1.5 Expected Points
  * FGA - Success  
    + Made FG 
    + Kick Success Probability  
    + 3 Expected Points
  * FGA - Fail  
    + Missed FG, opponent takes over at their own 40 yard line  
    + (1 - Kick Success Probability)  
    + -2.0 Expected Points
 
The following table shows the expected points for going for a first down compared to the expected points for kicking a field goal with a certain kicker.
 
```{r}
fg_decision <- data.frame(
  Kicker = c("-", "J.Tucker", "W.Lutz", "N.Novak", "D.Akers"),
  Decision = c("Go For It", "FGA", "FGA", "FGA", "FGA"),
  EP = c(.58*4 + .42*-1.5, 
         (kick_group_glmer %>% filter(kicker_player_name == "J.Tucker" & kick_distance == 50))$prob*3 + (1 - (kick_group_glmer %>% filter(kicker_player_name == "J.Tucker" & kick_distance == 50))$prob)*-2,
         (kick_group_glmer %>% filter(kicker_player_name == "W.Lutz" & kick_distance == 50))$prob*3 + (1 - (kick_group_glmer %>% filter(kicker_player_name == "W.Lutz" & kick_distance == 50))$prob)*-2,
         (kick_group_glmer %>% filter(kicker_player_name == "N.Novak" & kick_distance == 50))$prob*3 + (1 - (kick_group_glmer %>% filter(kicker_player_name == "N.Novak" & kick_distance == 50))$prob)*-2,
         (kick_group_glmer %>% filter(kicker_player_name == "D.Akers" & kick_distance == 50))$prob*3 + (1 - (kick_group_glmer %>% filter(kicker_player_name == "D.Akers" & kick_distance == 50))$prob)*-2)
) %>% 
  arrange(desc(EP))

datatable(
  fg_decision,
  rownames = FALSE,
  options = list(
    dom = "t",
    columnDefs = list(list(className = 'dt-center', targets = 0:2))
  )) %>% 
DT::formatRound("EP", digits = 2)
```

In this situation and with Justin Tucker as your kicker, you can maximize your expected points by attempting the field goal. With Will Lutz, the expected points are roughly even. With Nick Novak or David Akers, you maximize your expected points by going for the first down. This model allows you do this kind of analysis with any kicker, no matter their experience, to make an informed decision.  
  
One caveat, it is likely that a team would be interested in maximizing not their expected points, but their win probability. I'm not showing here whether maximizing expecting points is the optimal strategy in that regard. Even with Akers kicking, a team would have better chance of *scoring* on a field goal attempt than going for it, which may maximize a team's win probability in a tie game. The scope of that analysis is beyond this example. 

## Final Remarks

The final product from this analysis is two models. The first one estimates the expected probability of a field goal when we know the kick distance and some weather conditions. This is useful general analysis, and allows us to compare the previous success of kickers compared to what was expected from an average kicker using FOX. You can find the production code for this model to generate a table with xFG probabilities for all reasonable kicking conditions in `~/Production/xfg_general.R`.  
  
The second model included the player effect, allowing us to include the ability of the kicker in the model. This improved the accuracy of the model and would be applicable when it comes to in game decision making when a team is making a decision with a known kicker. The production code can be found at `~/Production/xfg_kicker_specific.R`, where you can generate of table of xFG probabilities for any kicker.  
  


